{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean Retrieval Model Implementation\n",
    "This notebook implements the Boolean Retrieval Model with inverted index and query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ehtis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Query Results (deep AND learning): [83, 84, 85, 86, 127, 145, 156, 162, 164, 168, 172, 176, 186, 192, 193, 198, 199, 202, 251, 260, 273, 274, 275, 276, 281, 286, 287, 290, 292, 302, 304, 305, 306, 312, 313, 314, 329, 330, 336, 339, 340, 351, 358, 370, 383]\n",
      "Proximity Query Results (neural information / 2): []\n",
      "Setup Complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to load stopwords\n",
    "def load_stopwords(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return set(f.read().splitlines())\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text, stopwords):\n",
    "    ps = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return [ps.stem(word) for word in tokens if word.isalpha() and word not in stopwords]\n",
    "\n",
    "# Function to build inverted and positional indexes\n",
    "def build_indexes(abstracts_folder, stopwords):\n",
    "    inverted_index = defaultdict(set)  \n",
    "    positional_index = defaultdict(lambda: defaultdict(list))  \n",
    "\n",
    "    for doc_id, filename in enumerate(sorted(os.listdir(abstracts_folder))):\n",
    "        file_path = os.path.join(abstracts_folder, filename)\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors=\"ignore\") as file:  # Ignore decoding errors\n",
    "                text = file.read()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading {file_path}: {e}\")\n",
    "            continue  # Skip problematic files\n",
    "\n",
    "        words = preprocess_text(text, stopwords)\n",
    "\n",
    "        for pos, word in enumerate(words):\n",
    "            inverted_index[word].add(doc_id)\n",
    "            positional_index[word][doc_id].append(pos)\n",
    "\n",
    "    return inverted_index, positional_index\n",
    "\n",
    "\n",
    "# Function to process Boolean queries\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()  # Define stemmer globally\n",
    "\n",
    "def boolean_query_processing(query, inverted_index, total_docs):\n",
    "    query = query.lower().split()\n",
    "    result_docs = set(range(total_docs))\n",
    "    operator = None\n",
    "    current_set = set()\n",
    "\n",
    "    for term in query:\n",
    "        if term in {\"and\", \"or\", \"not\"}:\n",
    "            operator = term\n",
    "        else:\n",
    "            stemmed_term = ps.stem(term)  # Convert query term to its stemmed form\n",
    "            term_docs = inverted_index.get(stemmed_term, set())\n",
    "\n",
    "            if operator == \"not\":\n",
    "                term_docs = result_docs - term_docs\n",
    "            elif operator == \"or\":\n",
    "                current_set |= term_docs\n",
    "            elif operator == \"and\":\n",
    "                current_set &= term_docs\n",
    "            else:\n",
    "                current_set = term_docs\n",
    "\n",
    "    return sorted(current_set)\n",
    "\n",
    "\n",
    "# Function to handle proximity queries (word1 word2 / k)\n",
    "def proximity_query(word1, word2, k, positional_index):\n",
    "    result_docs = set()\n",
    "    \n",
    "    if word1 in positional_index and word2 in positional_index:\n",
    "        docs1, docs2 = positional_index[word1], positional_index[word2]\n",
    "\n",
    "        for doc in docs1.keys() & docs2.keys():  # Find common documents\n",
    "            positions1 = docs1[doc]\n",
    "            positions2 = docs2[doc]\n",
    "\n",
    "            for p1 in positions1:\n",
    "                if any(abs(p1 - p2) <= k for p2 in positions2):\n",
    "                    result_docs.add(doc)\n",
    "                    break\n",
    "\n",
    "    return sorted(result_docs)\n",
    "\n",
    "# Load stopwords\n",
    "stopwords = load_stopwords(r\"C:\\Users\\ehtis\\Downloads\\Boolean_Retrieval_Model_Complete\\data\\Stopword-List.txt\")\n",
    "\n",
    "# Define dataset folder (Abstracts)\n",
    "abstracts_folder = r\"C:\\Users\\ehtis\\Downloads\\Boolean_Retrieval_Model_Complete\\data\\Abstracts\"\n",
    "\n",
    "# Build indexes\n",
    "inverted_index, positional_index = build_indexes(abstracts_folder, stopwords)\n",
    "\n",
    "# Testing with a Boolean Query\n",
    "query = \"deep AND learning\"\n",
    "boolean_results = boolean_query_processing(query, inverted_index, len(os.listdir(abstracts_folder)))\n",
    "\n",
    "# Testing with a Proximity Query\n",
    "proximity_results = proximity_query(\"neural\", \"information\", 2, positional_index)\n",
    "\n",
    "print(f\"Boolean Query Results ({query}): {boolean_results}\")\n",
    "print(f\"Proximity Query Results (neural information / 2): {proximity_results}\")\n",
    "\n",
    "print(\"Setup Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Query Results (deep AND learning): [83, 84, 85, 86, 127, 145, 156, 162, 164, 168, 172, 176, 186, 192, 193, 198, 199, 202, 251, 260, 273, 274, 275, 276, 281, 286, 287, 290, 292, 302, 304, 305, 306, 312, 313, 314, 329, 330, 336, 339, 340, 351, 358, 370, 383]\n"
     ]
    }
   ],
   "source": [
    "query = \"deep AND learning\"\n",
    "boolean_results = boolean_query_processing(query, inverted_index, len(os.listdir(abstracts_folder)))\n",
    "\n",
    "print(f\"Boolean Query Results ({query}): {boolean_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Query Results (machine OR learning AND NOT vision): [0, 1, 2, 4, 5, 6, 7, 9, 11, 12, 13, 15, 16, 19, 21, 23, 26, 28, 29, 30, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 64, 65, 67, 69, 73, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 97, 98, 100, 101, 107, 111, 112, 113, 114, 118, 119, 120, 121, 122, 124, 126, 127, 128, 134, 140, 141, 143, 144, 145, 148, 151, 152, 153, 155, 156, 157, 158, 159, 160, 162, 164, 165, 167, 168, 169, 170, 172, 173, 174, 176, 177, 179, 180, 181, 182, 183, 184, 186, 188, 189, 190, 191, 192, 193, 195, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 216, 218, 221, 223, 225, 226, 229, 231, 235, 236, 237, 238, 239, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 264, 265, 266, 267, 272, 273, 274, 275, 276, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 308, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 333, 335, 336, 339, 340, 344, 345, 347, 351, 353, 355, 357, 358, 364, 365, 368, 369, 370, 372, 373, 374, 376, 377, 378, 380, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 395, 396, 398, 402, 405, 407, 411, 412, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 436, 437, 438, 439, 440, 441, 444, 445, 446]\n"
     ]
    }
   ],
   "source": [
    "query = \"machine OR learning AND NOT vision\"\n",
    "boolean_results = boolean_query_processing(query, inverted_index, len(os.listdir(abstracts_folder)))\n",
    "\n",
    "print(f\"Boolean Query Results ({query}): {boolean_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Inverted Index (first 10 terms):\n",
      "ensembl: {0, 7, 396, 144, 24, 281, 174, 436, 181, 309, 187, 318, 205, 80, 222, 362, 235, 236, 109, 111, 245, 253}\n",
      "statist: {0, 384, 386, 387, 4, 263, 15, 143, 271, 18, 19, 405, 25, 284, 32, 417, 418, 298, 45, 173, 301, 51, 53, 56, 440, 188, 317, 63, 65, 326, 204, 78, 79, 340, 345, 356, 101, 104, 105, 366, 367, 368, 115, 244, 117, 246, 376, 121}\n",
      "and: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447}\n",
      "heurist: {0, 41, 175, 83, 373, 441, 349, 127}\n",
      "model: {0, 1, 5, 6, 7, 12, 13, 16, 17, 20, 22, 24, 25, 34, 35, 36, 37, 38, 39, 41, 42, 44, 46, 48, 49, 51, 52, 53, 54, 55, 58, 60, 63, 64, 65, 66, 67, 69, 71, 72, 73, 74, 75, 77, 87, 89, 92, 98, 99, 100, 101, 106, 107, 108, 109, 111, 112, 114, 115, 117, 118, 120, 122, 127, 130, 131, 134, 136, 142, 143, 144, 146, 147, 148, 150, 152, 153, 155, 157, 159, 161, 162, 163, 168, 169, 170, 171, 172, 173, 175, 179, 180, 185, 191, 192, 197, 199, 205, 206, 210, 211, 212, 213, 216, 217, 218, 219, 220, 221, 222, 227, 229, 230, 231, 237, 239, 244, 245, 246, 247, 248, 249, 250, 251, 253, 259, 262, 263, 267, 271, 272, 274, 278, 280, 284, 286, 289, 290, 291, 292, 297, 298, 300, 301, 302, 304, 306, 312, 313, 316, 317, 322, 323, 327, 328, 329, 331, 333, 335, 340, 341, 342, 343, 344, 349, 350, 351, 352, 353, 354, 355, 356, 362, 363, 366, 367, 368, 369, 372, 374, 375, 376, 377, 379, 381, 384, 385, 386, 389, 393, 395, 396, 398, 399, 403, 406, 407, 410, 411, 416, 417, 419, 423, 425, 433, 436, 437, 440, 446, 447}\n",
      "unsupervis: {0, 128, 383, 8, 138, 282, 41, 176, 54, 56, 186, 189, 445, 195, 199, 329, 83, 86, 90, 234, 113, 254, 255}\n",
      "word: {0, 288, 227, 389, 263, 391, 424, 362, 171, 172, 395, 339, 53, 437, 189}\n",
      "align: {0, 2, 326, 234, 175, 240, 287}\n",
      "learn: {0, 1, 2, 5, 6, 7, 11, 13, 15, 16, 19, 21, 23, 26, 28, 33, 34, 35, 37, 38, 39, 41, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 64, 65, 67, 69, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 97, 98, 100, 101, 107, 111, 112, 113, 114, 118, 119, 120, 121, 122, 124, 126, 127, 128, 134, 140, 141, 143, 144, 145, 148, 151, 152, 153, 155, 156, 157, 158, 159, 160, 162, 164, 165, 167, 168, 169, 170, 172, 173, 174, 176, 177, 179, 180, 181, 182, 184, 186, 188, 189, 190, 191, 192, 193, 195, 198, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 216, 218, 221, 223, 225, 226, 229, 231, 235, 236, 237, 238, 239, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 264, 265, 266, 267, 273, 274, 275, 276, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 308, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 333, 335, 336, 339, 340, 344, 345, 347, 351, 353, 355, 357, 358, 364, 368, 369, 370, 372, 373, 374, 376, 377, 378, 380, 382, 383, 384, 385, 387, 388, 389, 392, 393, 395, 396, 398, 402, 405, 407, 411, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 436, 438, 439, 440, 441, 444, 445, 446}\n",
      "need: {0, 2, 131, 389, 8, 265, 10, 267, 12, 21, 149, 23, 25, 287, 288, 419, 293, 41, 297, 299, 300, 431, 176, 435, 437, 439, 317, 62, 193, 72, 335, 80, 210, 83, 212, 88, 216, 345, 92, 225, 226, 357, 236, 237, 114, 245}\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample of Inverted Index (first 10 terms):\")\n",
    "for term, doc_list in list(inverted_index.items())[:10]:  # Show first 10 words\n",
    "    print(f\"{term}: {doc_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Index: ['ensembl', 'statist', 'and', 'heurist', 'model', 'unsupervis', 'word', 'align', 'learn', 'need', 'larg', 'amount', 'of', 'train', 'data', 'while', 'they', 'weak', 'corpora', 'thi', 'paper', 'propos', 'new', 'approach', 'hybrid', 'techniqu', 'use', 'method', 'algorithm', 'three', 'base', 'sever', 'round', 'gener', 'weigh', 'scheme', 'resampl', 'vote', 'score', 'consid', 'aggreg', 'underli', 'studi', 'includ', 'ibm', 'dice', 'measur', 'our', 'experiment', 'result']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words in Index:\", list(inverted_index.keys())[:50])  # Show first 50 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'learn' exists in the index: {0, 1, 2, 5, 6, 7, 11, 13, 15, 16, 19, 21, 23, 26, 28, 33, 34, 35, 37, 38, 39, 41, 43, 44, 47, 48, 49, 50, 51, 52, 53, 54, 56, 58, 59, 60, 61, 62, 64, 65, 67, 69, 74, 75, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 97, 98, 100, 101, 107, 111, 112, 113, 114, 118, 119, 120, 121, 122, 124, 126, 127, 128, 134, 140, 141, 143, 144, 145, 148, 151, 152, 153, 155, 156, 157, 158, 159, 160, 162, 164, 165, 167, 168, 169, 170, 172, 173, 174, 176, 177, 179, 180, 181, 182, 184, 186, 188, 189, 190, 191, 192, 193, 195, 198, 199, 200, 202, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 216, 218, 221, 223, 225, 226, 229, 231, 235, 236, 237, 238, 239, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 264, 265, 266, 267, 273, 274, 275, 276, 281, 282, 283, 284, 285, 286, 287, 288, 290, 291, 292, 293, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 308, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 333, 335, 336, 339, 340, 344, 345, 347, 351, 353, 355, 357, 358, 364, 368, 369, 370, 372, 373, 374, 376, 377, 378, 380, 382, 383, 384, 385, 387, 388, 389, 392, 393, 395, 396, 398, 402, 405, 407, 411, 414, 415, 416, 417, 418, 419, 421, 422, 423, 424, 425, 426, 436, 438, 439, 440, 441, 444, 445, 446}\n"
     ]
    }
   ],
   "source": [
    "search_word = \"learn\"  \n",
    "if search_word in inverted_index:\n",
    "    print(f\"✅ '{search_word}' exists in the index:\", inverted_index[search_word])\n",
    "else:\n",
    "    print(f\"❌ '{search_word}' NOT found in the index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Query Results (deep): [83, 84, 85, 86, 127, 145, 156, 162, 164, 168, 172, 176, 186, 192, 193, 198, 199, 202, 251, 260, 273, 274, 275, 276, 281, 286, 287, 290, 292, 302, 304, 305, 306, 312, 313, 314, 329, 330, 336, 339, 340, 351, 358, 370, 383]\n"
     ]
    }
   ],
   "source": [
    "query = \"deep\"\n",
    "boolean_results = boolean_query_processing(query, inverted_index, len(os.listdir(abstracts_folder)))\n",
    "\n",
    "print(f\"Boolean Query Results ({query}): {boolean_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
